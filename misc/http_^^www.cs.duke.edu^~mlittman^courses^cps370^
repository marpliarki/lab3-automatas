Date: Wed, 20 Nov 1996 22:15:54 GMT
Server: NCSA/1.5.1
Last-modified: Thu, 07 Nov 1996 22:14:29 GMT
Content-type: text/html
Content-length: 21116

<head>
<TITLE>CPS 370 - FALL 1996 </TITLE>
<LINK REV="made" HREF="mailto:mlittman@cs.duke.edu">
</head>
<body>
 
<p>

<center>

<TABLE BORDER=10 CELLSPACING=5 >
 <TR>
  <TD align=center colspan=4>
   <!WA0><img src="http://www.cs.duke.edu/~mlittman/courses/cps370/cps-t.gif" alt="CPS370">
  </TD>
 </TR>
 <TR></TR>
 <TR>
  <TD></TD>
  <TD ALIGN=center>
   <H1>Fall 1996</H1>
  </TD>
  <TD align=center></TD>
  <TD ALIGN=center>
   <H1>Planning Under Uncertainty</H1>
  </TD>
 </TR>
</TABLE>

</center>


<p>
<center>
[ <!WA1><A href="#background"> Background </A> |
<!WA2><A href="#grading"> Grading </A> |
<!WA3><A href="#outline"> Outline </A> |
<!WA4><A href="#projects"> Projects </A> ]
</center> <p>
<center>
<!WA5><a href="#now">Jump to current place in outline...</a>
</center>
<hr>

<p><!WA6><a href="http://www.cs.duke.edu/~mlittman/courses/cps370/syllabus">Evolving Syllabus...</a> <p>

<a name="background">
<h2> Background </h2>

This seminar will introduce students to an exciting area of research
that draws upon results from operations research (Markov decision
processes), machine learning (reinforcement learning), and traditional
AI (planning) to attack problems with great scientific and commercial
potential.  We will read and discuss a handful of recent papers that
will give students an appreciation for the state of the art.  Students
will undertake a group research project to solidify their
understanding of the basic concepts and perhaps to contribute to the
field. <p>

I will make a number of presentations to introduce students to the
background material necessary to read the research papers---only
undergraduate-level computer science knowledge and basic probability
theory and calculus will be assumed.  I think there are useful
contributions to be made by researchers in AI, algorithms, complexity,
numerical methods, and systems, and I think people in all these areas
would find some useful information in the seminar.  Everyone's welcome!

<h3> Instructor </h3>

<!WA7><A href="http://www.cs.duke.edu/~mlittman">Michael L. Littman</a>
<ul>
<li>Office: D209 LSRC
<li>Phone: 660-6537
<li>Email: <!WA8><a
href="mailto:mlittman@cs.duke.edu">mlittman@cs.duke.edu</a>
<li>Office hours: TBA
</ul>

<h3> Description </h3>

Research in planning, making a sequence of choices to achieve some
goal, has been a mainstay of artificial intelligence (AI) for many
years.  Traditionally, the decision-making models that have been
studied admit no uncertainty whatsoever---every aspect of the world
that is relevant to the generation and execution of a plan is known in
advance.  In contrast, work in operations research (OR) has focussed
on the uncertainty of actions but uses an impoverished representation
for specifying planning problems.  <p>

The purpose of this seminar is to explore some middle ground between
these two well-studied extremes with the hope of understanding how we
might create systems that can reason efficiently about plans in
complex, uncertain worlds.  We will review the foundational results
from AI and OR and read a series of papers written over the last few
years that have begun to bridge the gap.


<h3>Philosophy</h3>

There are basically two or three papers on different approaches to the
same basic problem that I'd like people to read and understand.  These
papers are quite recent and represent active areas of research that
have been maturing quite quickly over the last few years.  As a
result, to get a deep appreciation for this work, we will need to read
a number of papers that introduce the necessary background. <p>

My approach to organizing the seminar will be to try to keep the
assigned reading to a minimum and to ask students to concentrate on
understanding the state of the field and on identifying the important
open research questions.


<h3>Prerequisites</h3>

The seminar should be accessible to any advanced computer science
student.  My goal is to introduce critical background material as the
need arises. <p>

Nonetheless, we need some common ground to begin.  I will assume that
students are familiar with programming (any language), algorithm
analysis (big O notation), calculus (derivates of multivariate
functions), and probability theory (conditional probabilities).

<h3>Postrequisites</h3>

In addition to exploring the question of how to create plans that are
effective in uncertain environments, there are a number of other
important topics that students will learn about in this seminar:
students will be exposed to Markov decision processes, dynamic
programming, linear programming, temporal difference learning,
supervised function approximation, gradient descent and neural
networks, STRIPS rules, and partial order planning.

<hr>
<h2> <A NAME="grading"> Grading </h2>

The grading policy is designed to stimulate students to think about
some of the important issues in this area.  Class grade will be based
on:

<UL>
<LI> class participation (25%),
<LI> short homework assignments (25%), and
<LI> a final project (50%).
</UL>

<hr>

<A NAME="outline">
<h2>Outline</h2>

<h3>Organization Meeting</h3>

On Thursday, September 5th (D344, 2:30-3:00), we'll get together to
discuss the best time to schedule the class.  If you can't make the
meeting, please send me email (<!WA9><a
href="mailto:mlittman@cs.duke.edu">mlittman@cs.duke.edu</a>).


<h3>Introduction</h3>

What is planning?  What is uncertainty?  What are some applications of
planning under uncertainty?  I'll lay out the space of issues and
describe the part of the space we'll explore. <p>

<!WA10><img src="http://www.cs.duke.edu/~mlittman/courses/cps370/book.gif"> Michael Lederman Littman.  Algorithms for
Sequential Decision Making.  Ph.D. dissertation and Technical Report
CS-96-09, Brown University, Department of Computer Science,
Providence, RI, March 1996.  Chapter 1: Introduction.  (<!WA11><a
href="http://www.cs.duke.edu/~mlittman/courses/cps370/papers/littman-chap1.ps">local postscript</a>, <!WA12><a
href="http://www.cs.duke.edu/~mlittman/courses/cps370/papers/littman-bib.ps">local bibliography postscript</a>)


<h3>Markov Decision Processes</h3>

I'll introduce the MDP model, which is a formal specification for the
particular problem we will be examining.  I'll describe the
fundamental concepts (states, actions, transitions, rewards,
discounting, horizons), results (existence and dominance of optimal
value function, optimal greedy policies), and the algorithms (value
iteration, policy iteration, modified policy iteration, linear
programming). <p>

In a sense, these algorithms completely solve the problem of planning
under uncertainty.  The rest of the seminar is concerned with solving
MDPs more efficiently by exploiting additional structure present in
some instances. <p>

<!WA13><img src="http://www.cs.duke.edu/~mlittman/courses/cps370/book.gif"> Michael L. Littman, Thomas L. Dean, and Leslie Pack
Kaelbling.  On the complexity of solving Markov decision problems.  In
<em>Proceedings of the Eleventh Annual Conference on Uncertainty in
Artificial Intelligence (UAI--95)</em>, Montreal, Quebec, Canada,
1995. (<!WA14><a
href="http://www.cs.duke.edu/~mlittman/papers/mdp-complexity.ps">postscript</a>)
<p>

Homework: Represent a complex domain as an MDP. <p>

Slides: <!WA15><a href="http://www.cs.duke.edu/~mlittman/courses/cps370/slides/lect2.fm.ps">postscript</a> <p>


<h3>Accelerating Solutions to MDPs</h3>

One class of algorithms for solving MDPs more quickly restricts
value-iteration updates to states that are likely to benefit from
additional computational resources. <p>

Prioritized Sweeping uses a heuristic for measuring when updating the
value of a state is likely to be important for computing an
approximately optimal solution quickly. <p>

<!WA16><img src="http://www.cs.duke.edu/~mlittman/courses/cps370/book.gif"> Andrew W. Moore and Christopher G. Atkeson.  Prioritized
sweeping: Reinforcement learning with less data and less real
time. <em>Machine Learning</em>, 13:103, 1993.  (<!WA17><a
href="http://www.cs.cmu.edu/afs/cs/project/reinforcement/papers/moore.prisweep.ps.Z">compressed
postscript</a>) <p>

Real-time dynamic programming attempts to find a good approximate
policy quickly by focussing updates on states that are likely to be
visited.  <p>

<!WA18><img src="http://www.cs.duke.edu/~mlittman/courses/cps370/book.gif"> Andrew G. Barto, S. J. Bradtke and Satinder P. Singh.
Learning to act using real-time dynamic programming. <em>Artificial
Intelligence</em>, 72(